--- # deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: llm-graph-builder
  name: llm-graph-builder
#  namespace: llm-graph-builder
spec:
  selector:
    matchLabels:
      app: llm-graph-builder
  template:
    metadata:
      labels:
        app: llm-graph-builder
        app.kubernetes.io/name: llm-graph-builder # required for Homepage
    spec:
      containers:
        - name: llm-graph-builder-backend
          image: harbor.rockyroad.rocks/llm-graph-builder/llm-graph-builder-backend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8000 # backend defaults to 8000
              name: http-backend
              protocol: TCP
          resources:
            requests:
              cpu: 250m
              memory: 1000Mi
            limits:
              cpu: "500m"
              memory: "4000Mi"          
          envFrom:
            - configMapRef:
                name: backend-config
        - name: llm-graph-builder-frontend
          image: harbor.rockyroad.rocks/llm-graph-builder/llm-graph-builder-frontend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080 # frontend defaults to 8080
              name: http-frontend
              protocol: TCP
          resources:
            requests:
              cpu: 250m
              memory: 500Mi
            limits:
              cpu: "500m"
              memory: "1000Mi"          
          envFrom:
            - configMapRef:
                name: frontend-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-config
data:
  OPENAI_API_KEY: ""   #This is required if you are using openai embedding model
  EMBEDDING_MODEL: "all-MiniLM-L6-v2"  #this can be openai or vertexai or by default all-MiniLM-L6-v2
  RAGAS_EMBEDDING_MODEL: "openai"  #Keep blank if you want to use all-MiniLM-L6-v2 for ragas embeddings
  IS_EMBEDDING: "TRUE"
  KNN_MIN_SCORE: "0.94"
  # Enable Gemini (default is False) | Can be False or True
  GEMINI_ENABLED: "False"
  # Enable Google Cloud logs (default is False) | Can be False or True
  GCP_LOG_METRICS_ENABLED: "False"
  NUMBER_OF_CHUNKS_TO_COMBINE: "6"
  UPDATE_GRAPH_CHUNKS_PROCESSED: "20"
  NEO4J_URI: ""
  NEO4J_USERNAME: ""
  NEO4J_PASSWORD: ""
  NEO4J_DATABASE: ""
  AWS_ACCESS_KEY_ID:  ""
  AWS_SECRET_ACCESS_KEY: ""
  LANGCHAIN_API_KEY: ""
  LANGCHAIN_PROJECT: ""
  LANGCHAIN_TRACING_V2: ""
  LANGCHAIN_ENDPOINT: ""
  GCS_FILE_CACHE: "" #save the file into GCS or local, SHould be True or False
  NEO4J_USER_AGENT: ""
  ENABLE_USER_AGENT: ""
  LLM_MODEL_CONFIG_model_version: ""
  ENTITY_EMBEDDING: "TRUE"   # TRUE or FALSE based on whether to create embeddings for entities suitable for entity vector mode
  DUPLICATE_SCORE_VALUE: "0.97"
  DUPLICATE_TEXT_DISTANCE: "3"
  DEFAULT_DIFFBOT_CHAT_MODEL: "openai_gpt_4o"  #whichever model specified here , need to add config for that model in below format)
  #examples
  LLM_MODEL_CONFIG_ollama_qwen2.5-coder: "qwen2.5-coder,http://open-webui-ollama.ollama.svc.cluster.local:11434"
  #LLM_MODEL_CONFIG_openai_gpt_3.5: "gpt-3.5-turbo-0125,openai_api_key"
  #LLM_MODEL_CONFIG_openai_gpt_4o_mini: "gpt-4o-mini-2024-07-18,openai_api_key"
  #LLM_MODEL_CONFIG_openai_gpt_4o: "gpt-4o-2024-11-20,openai_api_key"
  #LLM_MODEL_CONFIG_openai-gpt-o3-mini: "o3-mini-2025-01-31,openai_api_key"
  #LLM_MODEL_CONFIG_gemini_1.5_pro: "gemini-1.5-pro-002"
  #LLM_MODEL_CONFIG_gemini_1.5_flash: "gemini-1.5-flash-002"
  #LLM_MODEL_CONFIG_gemini_2.0_flash: "gemini-2.0-flash-001"
  #LLM_MODEL_CONFIG_diffbot: "diffbot,diffbot_api_key"
  #LLM_MODEL_CONFIG_azure_ai_gpt_35: "azure_deployment_name,azure_endpoint or base_url,azure_api_key,api_version"
  #LLM_MODEL_CONFIG_azure_ai_gpt_4o: "gpt-4o,https://YOUR-ENDPOINT.openai.azure.com/,azure_api_key,api_version"
  #LLM_MODEL_CONFIG_groq_llama3_70b: "model_name,base_url,groq_api_key"
  #LLM_MODEL_CONFIG_anthropic_claude_3_5_sonnet: "model_name,anthropic_api_key"
  #LLM_MODEL_CONFIG_fireworks_llama_v3_70b: "model_name,fireworks_api_key"
  #LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet: "model_name,aws_access_key_id,aws_secret__access_key,region_name"
  #LLM_MODEL_CONFIG_ollama_llama3: "model_name,model_local_url"
  YOUTUBE_TRANSCRIPT_PROXY: "https://user:pass@domain:port"
  EFFECTIVE_SEARCH_RATIO: "5"
  GRAPH_CLEANUP_MODEL: "openai_gpt_4o"
  BEDROCK_EMBEDDING_MODEL: "model_name,aws_access_key,aws_secret_key,region_name"                       #model_name: "amazon.titan-embed-text-v1"
  #LLM_MODEL_CONFIG_bedrock_nova_micro_v1: "model_name,aws_access_key,aws_secret_key,region_name"        #model_name: "amazon.nova-micro-v1:0"
  #LLM_MODEL_CONFIG_bedrock_nova_lite_v1: "model_name,aws_access_key,aws_secret_key,region_name"         #model_name: "amazon.nova-lite-v1:0"
  #LLM_MODEL_CONFIG_bedrock_nova_pro_v1: "model_name,aws_access_key,aws_secret_key,region_name"          #model_name: "amazon.nova-pro-v1:0"
  #LLM_MODEL_CONFIG_fireworks_deepseek_r1: "model_name,fireworks_api_key"      #model_name: "accounts/fireworks/models/deepseek-r1"
  #LLM_MODEL_CONFIG_fireworks_deepseek_v3: "model_name,fireworks_api_key"      #model_name: "accounts/fireworks/models/deepseek-v3"
  MAX_TOKEN_CHUNK_SIZE: "2000" #Max token used to process/extract the file content.
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend-config
data:
  PLACEHOLDER: ""   
---
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  ports:
    - name: http-backend
      port: 8000
      protocol: TCP
      targetPort: http-backend
  selector:
    app: llm-graph-builder
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  ports:
    - name: http-frontend
      port: 8080
      protocol: TCP
      targetPort: http-frontend
  selector:
    app: llm-graph-builder
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: http-svc-np-back
spec:
  ports:
    - name: http-backend
      port: 8000
      protocol: TCP
      targetPort: http-backend
      nodePort: 30007
  selector:
    app: llm-graph-builder
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: http-svc-np-front
spec:
  ports:
    - name: http-frontend
      port: 8080
      protocol: TCP
      targetPort: http-frontend
      nodePort: 30008
  selector:
    app: llm-graph-builder
  type: NodePort
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    #kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: "letsencrypt-dns01-prod"
    #cert-manager.io/cluster-issuer: "letsencrypt-staging"
    kubernetes.io/ingress.class: "nginx"
    gethomepage.dev/description: Neo4j LLM Graph Builder
    gethomepage.dev/enabled: "true"
    gethomepage.dev/group: Handy Tools
    gethomepage.dev/icon: neo4j
    gethomepage.dev/name: Neo4j LLM Graph Builder
  name: llm-graph-builder
spec:
  rules:
    - host: llm-graph-builder.rockyroad.rocks
      http:
        paths:
          - backend:
              service:
                name: frontend
                port:
                  number: 8080
            path: /
            pathType: Prefix
  tls:
    - hosts:
        - llm-graph-builder.rockyroad.rocks
      secretName: llm-graph-builder-tls
